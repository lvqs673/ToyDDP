

**实现一个简单的分布式数据并行**
- 使用环形拓扑，三台主机
- 梯度的同步通过reduce-scatter和allgather实现


**实验任务使用LSTM进行时序预测**
- 时序数据为100个糖尿病患者的血糖变化
- *./data/blood_glucose*目录下有100位糖尿病患者的血糖数据，每个文件是一个患者的血糖数据
   - 每个文件中：第一列是血糖监测点编号，第二列是血糖浓度（单位mmol/L）
   - 每个监测点代表5分钟内的血糖浓度均值
- 实验预测目标是对未来半小时内的血糖浓度进行预测，因此需要预测6个监测点的值
- 每次预测参考前边5个小时的血糖变化情况，也就是前边60个监测点
- ***所以这个任务做的东西就是输入60个数据，输出6个数据***


**数据集构造**
1. 首先需要将序列数据转换为监督型数据。可以取大小为66的窗口在这个序列上滑动，这样可以得到若干条66维向量。
2. 对100个患者的序列数据均做如上处理，可以得到很多66维向量，将这些向量合并得到总的监督型数据。
3. 按照7:3划分训练集和测试集。

**模型**
- 模型包括两层，一层LSTM层和一层Dense层
    - LSTM层：使用两层的BiLSTM，隐藏层维度设置为50
    - Dense层：将LSTM层的输出映射到6维
- 模型输入：60维的向量
- 模型输出：6维的向量

**实验中记录的指标**
- 训练集上每个epoch的MSE、MAE（绘制Loss-Epoch曲线可视化，将单&多机的MSE绘制在一张图、MAE绘制在另一张图）（单机+多机）
- 测试集上每个epoch的MSE、MAE（绘制Loss-Epoch曲线可视化）（单机+多机）
- 对第100个患者的预测值（将该患者的实际血糖浓度和单机&多机的模型的预测血糖浓度绘制在同一张图中进行对比）（单机+多机）
- 单机下总的训练耗时；多机下总训练耗时、计算耗时、通信耗时：计算多机的加速比、有效计算时间（计算耗时/总训练时间）

**代码实现**
- *config.py*: 部分配置信息
- *dataset.py*: 血糖预测数据集相关
- *model.py*: 模型相关
- *utils.py*: 用到的一些通用函数
- *train_sn.py*: 单结点的训练脚本（sn为Single Node）
- *train_mn.py*: 多结点的训练脚本（mn为Multiple Node）

- *model_sn*: 该目录存储单节点训练的模型
- *data*: 该目录存储训练的数据和保存的结果


0 a0 b0 c0
1 a1 b1 c1
2 a2 b2 c2

reduce-scatter
第一轮
0(0)->1  1(1)->2  2(2)->0
0(2)<-2  1(0)<-0  2(1)<-1
0 a0    b0    c0+c2
1 a0+a1 b1    c1
2 a2    b1+b2 c2
第二轮
0(2)->1  1(0)->2  2(1)->0
0(1)<-2  1(2)<-0  2(0)<-1
0 a0       b0+b1+b2 c0+c2
1 a0+a1    b1       c0+c1+c2
2 a0+a1+a2 b1+b2    c2

allgather
第一轮：
0(1)->1  1(2)->2  2(0)->0
0(0)<-2  1(1)<-0  2(2)<-1
0 a0+a1+a2 b0+b1+b2 c0+c2
1 a0+a1    b0+b1+b2 c0+c1+c2
2 a0+a1+a2 b1+b2    c0+c1+c2
第二轮：
0(0)->1  1(1)->2  2(2)->0
0(2)<-2  1(0)<-0  2(1)<-1
0 a0+a1+a2 b0+b1+b2 c0+c1+c2
1 a0+a1+a2 b0+b1+b2 c0+c1+c2
2 a0+a1+a2 b0+b1+b2 c0+c1+c2
